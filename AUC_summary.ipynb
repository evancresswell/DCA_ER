{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ed548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from math import floor\n",
    "import timeit\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import auc, precision_recall_curve                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "DCA_ER_dir = '/data/cresswellclayec/DCA_ER' # Set DCA_ER directory\n",
    "biowulf_dir = '%s/biowulf_full' % DCA_ER_dir\n",
    "processed_data_dir = \"%s/protein_data/data_processing_output/\" % biowulf_dir\n",
    "\n",
    "pdb_path = \"/data/cresswellclayec/DCA_ER/biowulf_full/protein_data/metrics\"\n",
    "data_path = \"/data/cresswellclayec/DCA_ER/biowulf_full/protein_data/data_processing_output\"\n",
    "metric_dir = \"/data/cresswellclayec/DCA_ER/biowulf_full/protein_data/metrics\"\n",
    "\n",
    "\n",
    "ER_tprs = []\n",
    "ER_fprs = []\n",
    "PMF_tprs = []\n",
    "PMF_fprs = []\n",
    "PLM_tprs = []\n",
    "PLM_fprs = []\n",
    "MF_tprs = []\n",
    "MF_fprs = []\n",
    "\n",
    "ks_compares = []\n",
    "mc = []\n",
    "ER_bootstrap_aucs = []\n",
    "\n",
    "MSA_sizes = []\n",
    "n_cols = []\n",
    "n_seqs = []\n",
    "\n",
    "file_end = '_uni.npy'\n",
    "file_end = '_uni_ld5.npy'\n",
    "file_end = '.npy'\n",
    "\n",
    "bootstrapping = False\n",
    "if bootstrapping:\n",
    "    # Get list of files from completed auc-bootstrap files\n",
    "    boot_auc_files = list(Path(pdb_path).rglob(\"*bootstrap_aucs.npy\"))\n",
    "    boot_auc_files_str = [str(os.path.basename(path)) for path in boot_auc_files]\n",
    "    pfam_ids = [tp_str[:7] for tp_str in boot_auc_files_str] \n",
    "    pdb_ids = [tp_str[8:12] for tp_str in boot_auc_files_str] \n",
    "else:\n",
    "    # Get list of files from completed TP files\n",
    "    ks_files = list(Path(pdb_path).rglob(\"*method_comparison.pkl\"))\n",
    "    ks_files_str = [str(os.path.basename(path)) for path in ks_files]\n",
    "    pfam_ids = [ks_str[5:12] for ks_str in ks_files_str] \n",
    "    pdb_ids = [ks_str[:4] for ks_str in ks_files_str] \n",
    "    \n",
    "effective_seqs = []\n",
    "print(pfam_ids[:10])\n",
    "for i, pdb_id in enumerate(pdb_ids):\n",
    "    pfam_id = pfam_ids[i]\n",
    "    try:\n",
    "        ER_fp_file = \"%s/%s_%s_ER_fp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        ER_tp_file = \"%s/%s_%s_ER_tp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        ER_fp = np.load(ER_fp_file)\n",
    "        ER_tp = np.load(ER_tp_file)\n",
    "\n",
    "        PMF_fp_file = \"%s/%s_%s_PMF_fp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        PMF_tp_file = \"%s/%s_%s_PMF_tp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        PMF_fp = np.load(PMF_fp_file)\n",
    "        PMF_tp = np.load(PMF_tp_file)\n",
    "\n",
    "        PLM_fp_file = \"%s/%s_%s_PLM_fp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        PLM_tp_file = \"%s/%s_%s_PLM_tp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "        PLM_fp = np.load(PLM_fp_file)\n",
    "        PLM_tp = np.load(PLM_tp_file)\n",
    "\n",
    "    #     MF_fp_file = \"%s/%s_%s_MF_fp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "    #     MF_tp_file = \"%s/%s_%s_MF_tp%s\" % (pdb_path, pdb_id, pfam_id, file_end)\n",
    "    #     MF_fp = np.load(MF_fp_file)\n",
    "    #     MF_tp = np.load(MF_tp_file)   \n",
    "\n",
    "        if bootstrapping:\n",
    "            ER_bootstrap_file = \"%s/%s_%s_bootstrap_aucs.npy\" % (pdb_path, pfam_id, pdb_id)\n",
    "            ER_bootstrap = np.load(ER_bootstrap_file)\n",
    "\n",
    "        MSA_file = \"%s/%s_%s_preproc_msa.npy\" % (data_path, pfam_id, pdb_id)\n",
    "        MSA = np.load(MSA_file)\n",
    "\n",
    "        pfam_dimensions_file = \"%s%s_%s_pfam_dimensions.npy\" % (processed_data_dir, pdb_id, pfam_id)\n",
    "        pfam_dimensions = np.load(pfam_dimensions_file)\n",
    "\n",
    "    #     ks_file = \"%s/%s_%s_ks.pkl\" % (pdb_path, pdb_id, pfam_id)\n",
    "    #     with open(ks_file, \"rb\") as f:\n",
    "    #        ks = pickle.load(f)\n",
    "    #     f.close()\n",
    "    #     print(ks)\n",
    "\n",
    "        compare_file = \"%s/%s_%s_method_comparison.pkl\" % (pdb_path, pdb_id, pfam_id)\n",
    "        with open(compare_file, \"rb\") as f:\n",
    "            comparison = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    except(FileNotFoundError):\n",
    "        continue\n",
    "    PMF_fprs.append(PMF_fp)\n",
    "    PMF_tprs.append(PMF_tp)\n",
    "#     MF_fprs.append(MF_fp)\n",
    "#     MF_tprs.append(MF_tp)\n",
    "    ER_fprs.append(ER_fp)\n",
    "    ER_tprs.append(ER_tp)\n",
    "    PLM_fprs.append(PLM_fp)\n",
    "    PLM_tprs.append(PLM_tp)\n",
    "\n",
    "#     ks_compares.append(ks)\n",
    "    mc.append(comparison)\n",
    "    if bootstrapping:\n",
    "        ER_bootstrap_aucs.append(ER_bootstrap)\n",
    "\n",
    "    MSA_sizes.append(MSA.shape)\n",
    "    if len(pfam_dimensions)==7:\n",
    "        [n_col, n_seq, m_eff, ct_ER, ct_MF, ct_PMF, ct_PLM] = pfam_dimensions\n",
    "    elif len(pfam_dimensions)==6: # new pfam_dimensions created in run_method_comparison. we dont need MF..\n",
    "        [n_col, n_seq, m_eff, ct_ER, ct_PMF, ct_PLM] = pfam_dimensions\n",
    "    elif len(pfam_dimensions)==3:\n",
    "        [n_col, n_seq, m_eff] = pfam_dimensions\n",
    "    effective_seqs.append(m_eff)\n",
    "    n_cols.append(n_col)\n",
    "    n_seqs.append(n_seq)\n",
    "\n",
    "print(len(pfam_ids), ' Pfams plotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d0d04",
   "metadata": {},
   "source": [
    "# Plotting all ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d999671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_tprs = [ER_tprs, PMF_tprs, PLM_tprs, MF_tprs]\n",
    "# method_fprs = [ER_fprs, PMF_fprs, PLM_fprs, MF_fprs]\n",
    "# method_label = ['ER', 'PMF', 'PLM', 'MF']\n",
    "method_tprs = [ER_tprs, PMF_tprs, PLM_tprs]\n",
    "method_fprs = [ER_fprs, PMF_fprs, PLM_fprs]\n",
    "for tpr in method_tprs:\n",
    "    print(len(tpr))\n",
    "method_ids = ['ER', 'PMF', 'PLM']\n",
    "method_label =  ['ER', 'MF', 'PLM']\n",
    "method_color = 'brg'\n",
    "plt.figure(figsize=(38.0,12))\n",
    "method_aucs = []\n",
    "for i, method_tpr_data in enumerate(method_tprs):\n",
    "    method_fpr_data = method_fprs[i]\n",
    "    method_aucs.append([])\n",
    "    # Initalize ROC-curve tile\n",
    "    plt.subplot2grid((1,4),(0,i))\n",
    "    plt.title('%s ROC Summary' % method_label[i])\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "\n",
    "    for j, tpr in enumerate(method_tpr_data):\n",
    "\n",
    "        fpr = method_fpr_data[j]\n",
    "        method_aucs[-1].append(auc(fpr,tpr))\n",
    "        try:\n",
    "            plt.plot(fpr, tpr)\n",
    "        except:\n",
    "            pass\n",
    "plt.subplot2grid((1,4),(0,3))\n",
    "plt.title('Method Means ROC Summary')\n",
    "# plt.plot([0,1],[0,1],'k--')\n",
    "\n",
    "for i, method in enumerate(method_ids[:1]):\n",
    "    avg_fpr = np.load('%s/%s_avg_fpr.npy' % (metric_dir, method))\n",
    "    avg_tpr = np.load('%s/%s_avg_tpr.npy' % (metric_dir, method))\n",
    "    print(avg_tpr)\n",
    "    print(avg_fpr)\n",
    "    plt.plot(avg_fpr, avg_tpr, label=method)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8355db",
   "metadata": {},
   "source": [
    "# Histogram of AUC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_hex = {\"red\": \"#e41a1c\", \"blue\": \"#2258A5\", \"green\": \"#349C55\", \"purple\": \"#984ea3\", \"orange\": \"#FF8B00\",\n",
    "                      \"yellow\": \"#ffff33\", \"grey\": \"#BBBBBB\"}\n",
    "colors_key = [\"blue\", \"orange\", \"green\"]\n",
    "\n",
    "\n",
    "\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.,5.))\n",
    "\n",
    "\n",
    "for i, method_auc in enumerate(method_aucs):\n",
    "    method = method_label[i]\n",
    "    plt.hist(method_auc, bins=25, alpha=.3, label = method, color=colors_hex[colors_key[i]])  # density=False would make counts\n",
    "    plt.hist(method_auc, bins=25, histtype='step', color=colors_hex[colors_key[i]], linewidth=1.4)  # density=False would make counts\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Protein Count', fontsize=14)\n",
    "plt.xlabel('AUC', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('method_AUC_hist')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6605ff",
   "metadata": {},
   "source": [
    "# Which Method is Best\n",
    "* make a dataframe for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a135b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot best Method\n",
    "max_auc_indices = []\n",
    "max_aucs = []\n",
    "auc_differences = []\n",
    "print(len(method_aucs[0]))\n",
    "for i, er_auc in enumerate(method_aucs[0]):\n",
    "    pmf_auc = method_aucs[1][i]\n",
    "    plm_auc = method_aucs[2][i]\n",
    "    auc_compare = [er_auc, pmf_auc, plm_auc]\n",
    "    max_auc = max(auc_compare)\n",
    "    max_aucs.append(max_auc)\n",
    "    max_auc_index = auc_compare.index(max_auc)\n",
    "    max_auc_indices.append(max_auc_index)\n",
    "    auc_differences.append(abs(max_auc - np.mean([auc for auc in auc_compare if auc!=max_auc])))\n",
    "    \n",
    "    if bootstrapping:\n",
    "        # confidence intervals\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, np.percentile(ER_bootstrap_aucs[i], p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, np.percentile(ER_bootstrap_aucs[i], p))\n",
    "        #print('ER auc =%f PLM auc=%f PMF auc=%f\\n%.1f confidence interval %.1f%% and %.1f%%' % \n",
    "        #      (er_auc, plm_auc, pmf_auc, alpha*100, lower*100, upper*100))\n",
    "        if max_auc_index == 0 and np.mean([auc for auc in auc_compare if auc!=max_auc]) < lower:\n",
    "            max_auc_indices.append(.25)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10.0,5.))\n",
    "ax = plt.subplot2grid((1,2),(0,0))\n",
    "ax.hist(max_auc_indices ,range=(0,2) )  # density=False would make counts\n",
    "\n",
    "ax.set_xticks([0,.25,1,2])\n",
    "ax.set_xticklabels(['ER', 'ER clear', 'PMF', 'PLM'])\n",
    "plt.subplot2grid((1,2), (0,1))\n",
    "plt.hist(auc_differences, bins = 100, range=(0,.3))  # density=False would make counts\n",
    "plt.title('Difference between best and mean of losers\\naverage: %f' % np.mean(auc_differences))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205918c",
   "metadata": {},
   "source": [
    "# Binning By Column Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f13e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_by(aucs, max_aucs, shapes, metric_label='cols', n_bins=5):\n",
    "    n_bins = 10\n",
    "    \n",
    "    # set up bins\n",
    "    binned_auc = []\n",
    "    binned_method = []\n",
    "    binned_range = []\n",
    "    for i in range(n_bins):\n",
    "        binned_auc.append([])\n",
    "        binned_method.append([])\n",
    "\n",
    "    # clear values with not enough sequences.\n",
    "    bad_indices = []\n",
    "    for i, msa_shape in enumerate(shapes):\n",
    "        if msa_shape[0] < 300:\n",
    "            # print('%d seqs not enough' % msa_shape[0])\n",
    "            bad_indices.append(i)\n",
    "    bad_indices = sorted(bad_indices, reverse=True)\n",
    "    for idx in bad_indices:\n",
    "        if idx < len(shapes):\n",
    "            shapes.pop(idx)\n",
    "            aucs.pop(idx)\n",
    "            max_aucs.pop(idx)\n",
    "            \n",
    "    # set up metric\n",
    "    if metric_label=='cols':\n",
    "        metric = [msa_shape[1] for msa_shape in shapes]\n",
    "    elif metric_label=='num_seq':\n",
    "        metric = [msa_shape[0] for msa_shape in shapes]\n",
    "    \n",
    "\n",
    "    # bin metric values\n",
    "    if metric_label=='cols':\n",
    "        bins = np.linspace(min(metric), max(metric), num=n_bins)\n",
    "    elif metric_label=='num_seq':\n",
    "        print(min(metric), max(metric))\n",
    "        bins = np.geomspace(min(metric), max(metric), num=n_bins)\n",
    "\n",
    "    for i, auc in enumerate(aucs):\n",
    "        for j, (lower, upper) in enumerate(zip(bins, bins[1:])):\n",
    "            if i==0:\n",
    "                binned_range.append((lower,upper))\n",
    "            if metric[i] >= lower and metric[i] <= upper:\n",
    "                binned_auc[j].append(auc)\n",
    "                binned_method[j].append(max_aucs[i])\n",
    "    return binned_auc, binned_method, binned_range\n",
    "\n",
    "\n",
    "\n",
    "print(';%d, %d, %d' % (len(max_aucs), len(max_auc_indices), len(MSA_sizes)))\n",
    "binned_auc, binned_method, binned_range = bin_by(max_aucs, max_auc_indices, MSA_sizes, metric_label='cols', n_bins=5)  \n",
    "print(binned_range)\n",
    "\n",
    "plt.figure(figsize=(26.0,12))\n",
    "for i, auc_bin in enumerate(binned_auc[:-1]):\n",
    "    method_bin = binned_method[i]\n",
    "    \n",
    "    ax = plt.subplot2grid((2,len(binned_auc)), (0,i))\n",
    "    ax.hist(method_bin ,range=(0,2) )  # density=False would make counts\n",
    "    ax.set_xticks([0,.25,1,2])\n",
    "    ax.set_xticklabels(['ER', 'ER clear', 'PMF', 'PLM'])\n",
    "    ax.set_title('%s\\n[%d,%d]' %('cols', binned_range[i][0], binned_range[i][1]))\n",
    "    plt.subplot2grid((2,len(binned_auc)),(1,i))\n",
    "    plt.hist(auc_bin, bins = 100)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "binned_auc, binned_method, binned_range = bin_by(max_aucs, max_auc_indices, MSA_sizes, metric_label='num_seq', n_bins=5)  \n",
    "print(binned_range)\n",
    "\n",
    "plt.figure(figsize=(26.0,12))\n",
    "for i, auc_bin in enumerate(binned_auc[:-1]):\n",
    "    method_bin = binned_method[i]\n",
    "    \n",
    "    ax = plt.subplot2grid((2,len(binned_auc)), (0,i))\n",
    "    ax.hist(method_bin ,range=(0,2) )  # density=False would make counts\n",
    "    ax.set_xticks([0,.25,1,2])\n",
    "    ax.set_xticklabels(['ER', 'ER clear', 'PMF', 'PLM'])\n",
    "    ax.set_title('%s\\n[%d,%d]' %('Sequences', binned_range[i][0], binned_range[i][1]))\n",
    "    plt.subplot2grid((2,len(binned_auc)),(1,i))\n",
    "    plt.hist(auc_bin, bins = 100)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "def gen_auc_df(aucs, max_aucs, shapes, pdb_ids, pfam_ids):\n",
    "    cols = [msa_shape[1] for msa_shape in shapes]\n",
    "    seqs = [msa_shape[0] for msa_shape in shapes]\n",
    "    best_method = []\n",
    "    for x in max_aucs:\n",
    "        if x==0:\n",
    "            best_method.append('ER')\n",
    "        elif x==1:\n",
    "            best_method.append('PMF')\n",
    "        elif x==2:\n",
    "            best_method.append('PLM')\n",
    "    zipped = list(zip(aucs, best_method, seqs, effective_seqs, cols, pdb_ids, pfam_ids))\n",
    "    df_labels = ['AUC', 'Best Method', '# Sequences', 'Effective Sequence #', '# Positions', 'PDB ID', 'Pfam ID']\n",
    "    df = pd.DataFrame(zipped, columns=df_labels)\n",
    "    return df\n",
    "\n",
    "auc_df = gen_auc_df(max_aucs, max_auc_indices, MSA_sizes, pdb_ids, pfam_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_df[auc_df.AUC==auc_df.AUC.max()])\n",
    "print(auc_df.loc[auc_df['PDB ID'] == '1zdr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7604b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def get_tp_val(fp_val, fpr, tpr):\n",
    "    for i, fp in enumerate(fpr):\n",
    "        if fp < fp_val:\n",
    "            continue\n",
    "        elif fp == fp_val:\n",
    "            return tpr[i]\n",
    "        elif tpr[i] == tpr[i-1]:\n",
    "            return tpr[i]\n",
    "        if i > 0:\n",
    "            d = np.sqrt(abs(fp - fpr[i-1])**2 + abs(tpr[i]-tpr[i-1])**2)\n",
    "            avg_p = abs(fp_val-fpr[i-1])/d\n",
    "            tp_val = tpr[i-1] + avg_p * abs(tpr[i]-tpr[i-1])\n",
    "        else:\n",
    "            d = np.sqrt(abs(fp - 0)**2 + abs(tpr[i]-0)**2)\n",
    "            avg_p = fp_val/d\n",
    "            tp_val = avg_p * tpr[i]\n",
    "        return tp_val\n",
    "\n",
    "def get_full_length_tpr(fprs, tprs, full_fpr, i):\n",
    "    tpr = [get_tp_val(fp_val, fprs[i], tprs[i]) for fp_val in full_fpr]\n",
    "    print('%d tpr full: ' % i,  len(tpr))\n",
    "    return tpr\n",
    "\n",
    "def get_average_roc(tprs, fprs):\n",
    "    average_fpr = fprs[0]\n",
    "    print(len(average_fpr))\n",
    "    # fill average fpr with all\n",
    "    for i, fpr in enumerate(fprs[1:]):\n",
    "        average_fpr = np.unique(np.sort(np.concatenate((average_fpr, fpr))))\n",
    "    print(len(average_fpr))\n",
    "\n",
    "    average_tpr = np.zeros(len(average_fpr))\n",
    "    tprs_full = Parallel(n_jobs = 14)(delayed(get_full_length_tpr)(fprs, tprs, average_fpr, i) for i in range(len(fprs)))\n",
    "    for i, fpr in enumerate(fprs):\n",
    "        average_tpr = np.add(np.array(tprs_full[i]), average_tpr)\n",
    "        #plt.plot(fprs[i], tprs[i])\n",
    "    average_tpr = average_tpr / len(fprs)\n",
    "    #plt.plot(average_fpr, average_tpr, lw=2.5)\n",
    "    return average_fpr, average_tpr, tprs_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96624aa",
   "metadata": {},
   "source": [
    "# KS Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot best Method\n",
    "from itertools import combinations\n",
    "method_ids = ['ER', 'PMF', 'PLM']\n",
    "\n",
    "method_combos =['%svs%s' % combo for combo in combinations(method_ids, 2)]\n",
    "ks_vals = {}\n",
    "p_vals = {}\n",
    "\n",
    "# asymptotic ks test, and z test run_from method_comparison.py\n",
    "ks_vals_asympt = {}\n",
    "p_vals_asympt = {}\n",
    "z = {}\n",
    "print(mc[0].keys())\n",
    "for ks_combo in mc[0].keys():\n",
    "    ks_vals[ks_combo] = []\n",
    "    p_vals[ks_combo] = []\n",
    "    ks_vals_asympt[ks_combo] = []\n",
    "    p_vals_asympt[ks_combo] = []\n",
    "    z[ks_combo] = []\n",
    "\n",
    "ER_combos = [combo for combo in method_combos if \"ER\" in combo]\n",
    "PLM_combos = [combo for combo in method_combos if \"PLM\" in combo]\n",
    "PMF_combos = [combo for combo in method_combos if \"MF\" in combo]\n",
    "ks_max = .2\n",
    "\n",
    "max_auc_indices = []\n",
    "max_aucs = []\n",
    "auc_differences = []\n",
    "for i, er_auc in enumerate(method_aucs[0]):\n",
    "    pmf_auc = method_aucs[1][i]\n",
    "    plm_auc = method_aucs[2][i]\n",
    "    auc_compare = [er_auc, pmf_auc, plm_auc]\n",
    "    max_auc = max(auc_compare)\n",
    "    max_aucs.append(max_auc)\n",
    "    max_auc_index = auc_compare.index(max_auc)\n",
    "    max_auc_indices.append(max_auc_index)\n",
    "    auc_differences.append(abs(max_auc - np.mean([auc for auc in auc_compare if auc!=max_auc])))\n",
    "    try: # during swarm simulations theres an issue with PMF vs MF keys.. should distiguish PYDCA MF (PMF)\n",
    "        ks = mc[i]\n",
    "        for ks_combo in ks.keys():\n",
    "            try:\n",
    "                ks_vals[ks_combo].append(ks[ks_combo][0])\n",
    "                p_vals[ks_combo].append(ks[ks_combo][1])\n",
    "                ks_vals_asympt[ks_combo].append(mc[i][ks_combo][0][0])\n",
    "                p_vals_asympt[ks_combo].append(mc[i][ks_combo][0][1])\n",
    "\n",
    "                z[ks_combo].append(mc[j][ks_combo][1][0][0])\n",
    "            except(IndexError):\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        if bootstrapping:\n",
    "            # confidence intervals\n",
    "            alpha = 0.95\n",
    "            p = ((1.0-alpha)/2.0) * 100\n",
    "            lower = max(0.0, np.percentile(ER_bootstrap_aucs[i], p))\n",
    "            p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "            upper = min(1.0, np.percentile(ER_bootstrap_aucs[i], p))\n",
    "            #print('ER auc =%f PLM auc=%f PMF auc=%f\\n%.1f confidence interval %.1f%% and %.1f%%' % \n",
    "            #      (er_auc, plm_auc, pmf_auc, alpha*100, lower*100, upper*100))\n",
    "\n",
    "        if max_auc_index == 0 and np.mean([ks[combo][0] for combo in ER_combos]) <= ks_max:\n",
    "            max_auc_indices.append(.25)\n",
    "        if max_auc_index == 1 and np.mean([ks[combo][0] for combo in PMF_combos]) <= ks_max:\n",
    "            max_auc_indices.append(1.25)\n",
    "        if max_auc_index == 2 and np.mean([ks[combo][0] for combo in PLM_combos]) <= ks_max:\n",
    "            max_auc_indices.append(2.25)\n",
    "    except(KeyError):\n",
    "        continue\n",
    "# print(ks_vals)\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: # takes a long time. ks-asymptotic is better anyways\n",
    "    plt.figure(figsize=(15.0,5.))\n",
    "    ax1=plt.subplot2grid((3,1), (0,0), colspan=1, rowspan=1)\n",
    "    ax2=plt.subplot2grid((3,1), (1,0), colspan=1, rowspan=1)\n",
    "    ax3=plt.subplot2grid((3,1), (2,0), colspan=1, rowspan=1)\n",
    "\n",
    "\n",
    "    for ks_combo in mc[0].keys():\n",
    "        ax1.hist(ks_vals[ks_combo], bins=25, alpha=.3, label =ks_combo)  # density=False would make counts\n",
    "        ax1.hist(ks_vals[ks_combo], bins=25, histtype='step', linewidth=1.4)  # density=False would make counts\n",
    "\n",
    "        ax2.hist(p_vals[ks_combo], bins=100, alpha=.3, label =ks_combo)  # density=False would make counts\n",
    "        ax2.hist(p_vals[ks_combo], bins=100, histtype='step', color='k', linewidth=1.4)  # density=False would make counts\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax1.set_title('KS statistic', fontsize = 14)\n",
    "    ax2.set_title('p-values', fontsize=14)\n",
    "\n",
    "    ax3.hist(max_auc_indices ,range=(0,2.5) )  # density=False would make counts\n",
    "    ax3.set_xticks([0,.25,1,1.25,2,2.25])\n",
    "    ax3.set_xticklabels(['ER', 'ER clear ks' , 'PMF', 'PMF clear ks', 'PLM', 'PLM clear ks'])\n",
    "    ax3.set_title('Method win count vs KS <.2 Method win count', fontsize=35)\n",
    "    #plt.setp(ax1.get_xticklabels(),fontsize=25)\n",
    "    #plt.setp(ax1.get_yticklabels(),fontsize=25)\n",
    "    #plt.setp(ax2.get_xticklabels(),fontsize=25)\n",
    "    #plt.setp(ax2.get_yticklabels(),fontsize=25)\n",
    "    #plt.setp(ax3.get_xticklabels(),fontsize=25)\n",
    "    ##plt.setp(ax3.get_yticklabels(),fontsize=25)\n",
    "    plt.savefig('ks_comparison.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a537751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asymptotic ks\n",
    "plt.figure(figsize=(5.,15.))\n",
    "ax1=plt.subplot2grid((3,1), (0,0), colspan=1, rowspan=1)\n",
    "ax2=plt.subplot2grid((3,1), (1,0), colspan=1, rowspan=1)\n",
    "ax3=plt.subplot2grid((3,1), (2,0), colspan=1, rowspan=1)\n",
    "\n",
    "\n",
    "        \n",
    "for ks_combo in mc[0].keys():\n",
    "    ax1.hist(ks_vals_asympt[ks_combo], bins=25, alpha=.3, label =ks_combo, )  # density=False would make counts\n",
    "    ax1.hist(ks_vals_asympt[ks_combo], bins=25, histtype='step', color='k', linewidth=1.4)  # density=False would make counts\n",
    "\n",
    "    ax2.hist(p_vals_asympt[ks_combo], log=True, bins=500, alpha=.3, label =ks_combo, linewidth=1.4)  # density=False would make counts\n",
    "    ax2.hist(p_vals_asympt[ks_combo], log=True, bins=500, histtype='step', color='k', linewidth=1.4)  # density=False would make counts\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_title('KS (asymptotic) statistic')\n",
    "ax2.set_title('p-values')\n",
    "\n",
    "ax3.hist(max_auc_indices ,range=(0,2.5) )  # density=False would make counts\n",
    "ax3.set_xticks([0,.25,1,1.25,2,2.25])\n",
    "ax3.set_xticklabels(['ER', 'ER clear ks' , 'PMF', 'PMF clear ks', 'PLM', 'PLM clear ks'])\n",
    "ax3.set_title('Method win count vs KS <.2 Method win count')\n",
    "plt.setp(ax1.get_xticklabels(),fontsize=14)\n",
    "plt.setp(ax1.get_yticklabels(),fontsize=14)\n",
    "plt.setp(ax2.get_xticklabels(),fontsize=14)\n",
    "plt.setp(ax2.get_yticklabels(),fontsize=14)\n",
    "plt.setp(ax3.get_xticklabels(),fontsize=14)\n",
    "plt.setp(ax3.get_yticklabels(),fontsize=14)\n",
    "plt.savefig('ks_asympt_comparison.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Manuscript KS- plots\n",
    "ER_ks_vals = []\n",
    "for ks_combo in mc[0].keys():\n",
    "    print(len(ks_vals_asympt[ks_combo]))\n",
    "    if 'ER' in ks_combo:\n",
    "        ER_ks_vals.extend(ks_vals_asympt[ks_combo])\n",
    "ER_ks_pvals = []\n",
    "for ks_combo in mc[0].keys():\n",
    "    print(len(p_vals_asympt[ks_combo]))\n",
    "    if 'ER' in ks_combo:\n",
    "        temp = []\n",
    "        for p in p_vals_asympt[ks_combo]:\n",
    "            if p!=0:\n",
    "                temp.append(np.log(p))\n",
    "        ER_ks_pvals.extend(temp)\n",
    "plt.figure(figsize=(5.,5.))\n",
    "ax1=plt.subplot2grid((1,1), (0,0), colspan=1, rowspan=1)\n",
    "ax1.hist(ER_ks_vals, bins=25)  # density=False would make counts\n",
    "plt.setp(ax1.get_xticklabels(),fontsize=14)\n",
    "plt.setp(ax1.get_yticklabels(),fontsize=14)\n",
    "ax1.set_xlabel('KS-Statistic', fontsize=14)\n",
    "ax1.set_ylabel('ER Comparison Count', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ER_ks_hist.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5.,5.))\n",
    "ax1=plt.subplot2grid((1,1), (0,0), colspan=1, rowspan=1)\n",
    "ax1.hist(ER_ks_pvals, bins=25)  # density=False would make counts\n",
    "ax1.set_xlabel('log(p)', fontsize=14)\n",
    "ax1.set_ylabel('Count', fontsize=14)\n",
    "plt.setp(ax1.get_xticklabels(),fontsize=14)\n",
    "\n",
    "plt.setp(ax1.get_yticklabels(),fontsize=14)\n",
    "plt.savefig('ER_kspval_hist.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26341fdc",
   "metadata": {},
   "source": [
    "# Z-test Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443c913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # plot best Method\n",
    "    z_lower = -2\n",
    "    for ks_combo in ks.keys():\n",
    "        print(len(z[ks_combo]))\n",
    "    max_auc_indices_z = []\n",
    "    for i, er_auc in enumerate(method_aucs[0]):\n",
    "        pmf_auc = method_aucs[1][i]\n",
    "        plm_auc = method_aucs[2][i]\n",
    "        auc_compare = [er_auc, pmf_auc, plm_auc]\n",
    "        max_auc = max(auc_compare)\n",
    "        max_auc_index = auc_compare.index(max_auc)\n",
    "        max_auc_indices_z.append(max_auc_index)\n",
    "\n",
    "\n",
    "        if max_auc_index == 0 and np.mean([z[ks_combo] for ks_combo in ks.keys() if '_MF' not in ks_combo and 'ER' in ks_combo]) < z_lower:\n",
    "            max_auc_indices_z.append(.25)\n",
    "        if max_auc_index == 1 and np.mean([z[ks_combo] for ks_combo in ks.keys() if '_MF' not in ks_combo and 'PMF' in ks_combo]) < z_lower:\n",
    "            max_auc_indices_z.append(1.25)\n",
    "        if max_auc_index == 2 and np.mean([z[ks_combo] for ks_combo in ks.keys() if '_MF' not in ks_combo and 'PLM' in ks_combo]) < z_lower:\n",
    "            max_auc_indices_z.append(2.25)\n",
    "\n",
    "    print('ER has %d clear winners' % len([val for val in max_auc_indices_z if val == .25]))\n",
    "    print('PMF has %d clear winners' % len([val for val in max_auc_indices_z if val == 1.25]))\n",
    "    print('PLM has %d clear winners' % len([val for val in max_auc_indices_z if val == 2.25]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf08925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "dark2 = cm = plt.get_cmap('jet') \n",
    "cNorm  = colors.Normalize(vmin=0, vmax=len(mc[0].keys()))\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=dark2)\n",
    "# asymptotic ks\n",
    "if 0:\n",
    "    # z-test comparison\n",
    "    plt.figure(figsize=(36.0,26))\n",
    "    ax1=plt.subplot2grid((2,1), (0,0), colspan=1, rowspan=1)\n",
    "\n",
    "\n",
    "    for i, ks_combo in enumerate(mc[0].keys()):\n",
    "        if i == 0:\n",
    "            _, bins,_ = ax1.hist(z[ks_combo], bins=150, alpha=.2, color=scalarMap.to_rgba(i))  # density=False would make counts\n",
    "            ax1.hist(z[ks_combo], bins=bins, histtype='step', color=scalarMap.to_rgba(i), linewidth=2, label =ks_combo, )  # density=False would make counts\n",
    "        else:\n",
    "            ax1.hist(z[ks_combo], bins=bins, alpha=.2, color=scalarMap.to_rgba(i) )  # density=False would make counts\n",
    "            ax1.hist(z[ks_combo], bins=bins, histtype='step', color=scalarMap.to_rgba(i), linewidth=2, label =ks_combo, )  # density=False would make counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax1.legend(fontsize=25)\n",
    "    ax1.set_title('Z-test Comparison', fontsize = 35)\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(),fontsize=25)\n",
    "    plt.setp(ax1.get_yticklabels(),fontsize=25)\n",
    "\n",
    "    ax1.set_xlim((-15,0))\n",
    "\n",
    "    ax3=plt.subplot2grid((2,1), (1,0), colspan=1, rowspan=1)\n",
    "\n",
    "\n",
    "    ax3.hist(max_auc_indices_z ,range=(0,2.5) )  # density=False would make counts\n",
    "    ax3.set_xticks([0,.25,1,1.25,2,2.25])\n",
    "    ax3.set_xticklabels(['ER', 'ER clear ks' , 'PMF', 'PMF clear ks', 'PLM', 'PLM clear ks'])\n",
    "    ax3.set_title('Method win count vs Z-test log(p-value)<%f Method win count' % z_lower, fontsize=35)\n",
    "\n",
    "    plt.setp(ax3.get_xticklabels(),fontsize=25)\n",
    "    plt.setp(ax3.get_yticklabels(),fontsize=25)\n",
    "    plt.savefig('z_comparison.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "ks_max = .2\n",
    "c_alpha = {.2:1.073, .14:1.138, .1:1.224, .05:1.358, .025:1.48, .01:1.628, .005:1.731, .001:1.949}\n",
    "alpha = .001\n",
    "from scipy.special import comb\n",
    "combos = [[combo for combo in method_combos if \"ER\" in combo], [combo for combo in method_combos if \"PLM\" in combo]\n",
    "          ,[combo for combo in method_combos if \"PMF\" in combo]]\n",
    "print(len(ks_vals_asympt['ERvsPMF']))\n",
    "n_cols_full = []\n",
    "n_seqs_full = []\n",
    "methods_full = []\n",
    "pdb_ids_full = []\n",
    "pfam_ids_full = []\n",
    "aucs_full = []\n",
    "neffs_full = []\n",
    "best_method_full = []\n",
    "auc_differences_full = []\n",
    "ks_clear_full = []\n",
    "for i, method_auc_data in enumerate(method_aucs):\n",
    "    ks_crit = c_alpha[alpha] * np.sqrt(2*comb(n_cols[i],2) / comb(n_cols[i],2)**2)\n",
    "\n",
    "    for j, auc in enumerate(method_auc_data):\n",
    "        pdb_ids_full.append(pdb_ids[j])\n",
    "        pfam_ids_full.append(pfam_ids[j])\n",
    "        methods_full.append(method_label[i])\n",
    "        aucs_full.append(auc)\n",
    "        n_cols_full.append(n_cols[j])\n",
    "        n_seqs_full.append(n_seqs[j])\n",
    "        neffs_full.append(effective_seqs[j])\n",
    "        \n",
    "\n",
    "        # Best method\n",
    "        er_auc = method_aucs[0][j]\n",
    "        pmf_auc = method_aucs[1][j]\n",
    "        plm_auc = method_aucs[2][j]\n",
    "        auc_compare = [er_auc, pmf_auc, plm_auc]\n",
    "        max_auc = max(auc_compare)\n",
    "        max_aucs.append(max_auc)\n",
    "        max_auc_index = auc_compare.index(max_auc)\n",
    "        if max_auc_index == 0:\n",
    "            best_method_full.append('ER')\n",
    "        elif max_auc_index == 1: \n",
    "            best_method_full.append('MF')\n",
    "        else:            \n",
    "            best_method_full.append('PLM')\n",
    "        \n",
    "        auc_differences_full.append(abs(max_auc - np.mean([auc for auc in auc_compare if auc!=max_auc])))\n",
    "\n",
    "        # KS\n",
    "        ks = mc[i]\n",
    "        try:\n",
    "            if max_auc_index == 0 and np.mean([ks_vals_asympt[combo][j] for combo in combos[0]]) >= ks_crit:\n",
    "                ks_clear_full.append(True)\n",
    "            elif max_auc_index == 1 and np.mean([ks_vals_asympt[combo][j] for combo in combos[1]]) >= ks_crit:\n",
    "                ks_clear_full.append(True)\n",
    "            elif max_auc_index == 2 and np.mean([ks_vals_asympt[combo][j] for combo in combos[2]]) >= ks_crit:\n",
    "                ks_clear_full.append(True)\n",
    "            else:\n",
    "                ks_clear_full.append(False)\n",
    "        except(IndexError):\n",
    "            print(j)\n",
    "method_comparison_df = pd.DataFrame(list(zip(n_cols_full, n_seqs_full, neffs_full, methods_full, best_method_full, pdb_ids_full, pfam_ids_full, aucs_full, ks_clear_full)),\n",
    "               columns =['Number of Columns', 'Number of Sequences', 'Effective Sequences', 'Method', 'Best Method', 'PDB ID', 'Pfam ID', 'AUC', 'KS-Mean Clear'])\n",
    "        \n",
    "# Stratify by Number of Columns\n",
    "method_comparison_df['Column Number Range'] = pd.cut(method_comparison_df['Number of Columns'],np.arange(min(method_comparison_df['Number of Columns']),max(method_comparison_df['Number of Columns']),step=200))\n",
    "\n",
    "# Stratify by Number of Sequences\n",
    "method_comparison_df['Sequence Number Range'] = pd.cut(method_comparison_df['Number of Sequences'],np.arange(min(method_comparison_df['Number of Sequences']),max(method_comparison_df['Number of Sequences']),step=10000))\n",
    "\n",
    "# Remove all proteins with less than 300 Sequences\n",
    "method_comparison_df = method_comparison_df.loc[method_comparison_df['Effective Sequences']  >= 150]\n",
    "\n",
    "# Stratify by Number of Sequences (log scale)\n",
    "log_min =np.log10(min(method_comparison_df['Effective Sequences']))\n",
    "log_max =np.log10(max(method_comparison_df['Effective Sequences']))\n",
    "print(log_min, log_max)\n",
    "\n",
    "log_ranges = np.linspace(log_min,log_max,num=4)\n",
    "method_comparison_df['Effective Sequences (log)'] = pd.cut(np.log10(method_comparison_df['Effective Sequences']),log_ranges)\n",
    "\n",
    "\n",
    "\n",
    "print( method_comparison_df.head())\n",
    "print(method_comparison_df.loc[method_comparison_df['PDB ID'] == '1zdr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_results = method_comparison_df.loc[method_comparison_df['Method'] == method_comparison_df['Best Method']]\n",
    "len(df_unique_results)\n",
    "print('ks mean clear proteins: ',len(df_unique_results.loc[df_unique_results['KS-Mean Clear']==True]))\n",
    "print('non-clear proteins: ',len(df_unique_results.loc[df_unique_results['KS-Mean Clear']==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfaa42",
   "metadata": {},
   "source": [
    "## Bin AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d478ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "#set seaborn plotting aesthetics\n",
    "\n",
    "# # bin AUC values for each method\n",
    "# plt.figure(figsize=(5.0,5.))\n",
    "# ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "x,y,hue = 'KS-Mean Clear', 'Method','Best Method'\n",
    "df_best_method_count= (df_unique_results [x].groupby(df_unique_results [hue]).value_counts().rename(y).reset_index())\n",
    "df_best_method_count = df_best_method_count.rename(columns={\"level_1\":x})\n",
    "print(df_best_method_count)\n",
    "#create stacked bar chart\n",
    "#df_best_method_count.set_index('Best Method').plot(kind='bar', stacked=True)\n",
    "\n",
    "# set plot style: grey grid in the background:\n",
    "#sns.set(style=\"darkgrid\")\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(5., 5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "sns.barplot(x=\"Best Method\", y=\"Method\",  hue=\"KS-Mean Clear\", data=df_best_method_count, palette=['lightblue', 'darkblue'], ax=ax)\n",
    "\n",
    "# # top bar -> sum all values(smoker=No and smoker=Yes) to find y position of the bars\n",
    "# total = df_unique_results.groupby('Method')['KS-Mean Clear'].sum().reset_index()\n",
    "# # bar chart 1 -> top bars (group of total group)\n",
    "# print(total)\n",
    "# bar1 = sns.barplot(x=\"Method\",  y=\"KS-Mean Clear\", data=total, color='lightblue', ax=ax)\n",
    "# # bottom bar ->  take only KS-Mean Clear values from the data\n",
    "# ks_clear = df_unique_results.loc[df_unique_results['KS-Mean Clear']==True]\n",
    "# print(len(ks_clear))\n",
    "# total_clear = ks_clear.groupby('Method')['KS-Mean Clear'].sum().reset_index()\n",
    "# print(total_clear)\n",
    "# # bar chart 2 -> bottom bars (group of 'KS-Mean Clear')\n",
    "# bar2 = sns.barplot(x=\"Method\", y=\"KS-Mean Clear\", data=total_clear,estimator=sum, ci=None,  color='darkblue', ax=ax)\n",
    "\n",
    "# # add legend\n",
    "# top_bar = mpatches.Patch(color='lightblue', label='Not a Clear Winner (KS-Mean)')\n",
    "# bottom_bar = mpatches.Patch(color='darkblue', label='Clear Winner (KS-Mean)')\n",
    "ax.set_ylabel('Count', fontsize = 14)\n",
    "ax.set_xlabel('Method', fontsize = 14)\n",
    "\n",
    "plt.legend(title='KS-Mean Clear')\n",
    "plt.savefig('BM_ksclear.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create Bars for Log scale Sequence Number ranges.\n",
    "x,y,hue = 'Method', 'Method','Best Method'\n",
    "df_method_count= (df_unique_results [x].groupby(df_unique_results [x]).value_counts())\n",
    "print(\"df_method_count: \\n\", df_method_count)\n",
    "\n",
    "\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "g1 = df_unique_results[x].value_counts().plot(kind='bar',\n",
    "                                        ax = ax,\n",
    "                                         color=[\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "# g1 = sns.barplot(x=x, data=df_method_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "g1.set_yscale('log')\n",
    "ax.set_ylabel('Protein Count', fontsize=14)\n",
    "ax.set_xlabel(x, fontsize=14)\n",
    "\n",
    "plt.savefig('BM_count.pdf')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "g1 = df_unique_results[x].value_counts().plot(kind='pie', autopct='%1.0f%%',fontsize=14,colors=[\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "# g1 = sns.barplot(x=x, data=df_method_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "\n",
    "plt.savefig('BM_count_pie.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b00b1",
   "metadata": {},
   "source": [
    "### Bin AUC (stratify by Number of Sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bars for Log scale Sequence Number ranges.\n",
    "x,y,hue = 'Effective Sequences (log)', 'Method','Best Method'\n",
    "df_seq_range_count= (df_unique_results [x].groupby(df_unique_results [hue]).value_counts().rename(y).reset_index())\n",
    "df_seq_range_count = df_seq_range_count.rename(columns={\"level_1\":x})\n",
    "num_seq_order = df_seq_range_count.loc[df_seq_range_count['Method']>10][x].sort_values().unique()\n",
    "\n",
    "\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "g = sns.barplot(x=x, y=y, hue=hue,order = num_seq_order, data=df_seq_range_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "g.set_yscale('log')\n",
    "ax.set_ylabel('Protein Count', fontsize=14)\n",
    "ax.set_xlabel(x, fontsize=14)\n",
    "plt.savefig('BMhist_nseq_logrange.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create Bars for Log scale Sequence Number ranges.\n",
    "x,y,hue = 'Effective Sequences (log)', 'Method','Best Method'\n",
    "df_seq_range_count= (df_unique_results [x].groupby(df_unique_results [hue]).value_counts().rename(y).reset_index())\n",
    "df_seq_range_count = df_seq_range_count.rename(columns={\"level_1\":x})\n",
    "num_seq_order = df_seq_range_count.loc[df_seq_range_count['Method']>10][x].sort_values().unique()\n",
    "\n",
    "\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "g1 = sns.barplot(x=x, y=y, hue=hue,order = num_seq_order, data=df_seq_range_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "ax.set_ylabel('Protein Count', fontsize=20)\n",
    "ax.set_xlabel(x, fontsize=20)\n",
    "xlabels = ax.get_xticklabels()\n",
    "ax.set_xticklabels(xlabels, rotation=20, ha='right')\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "g1.set_yscale('log')\n",
    "plt.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('BMhist_nseq_logrange_poster.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48280c16",
   "metadata": {},
   "source": [
    "### Bin AUC (stratify by Number of Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10448e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bars for Log scale Sequence Number ranges.\n",
    "x,y,hue = 'Column Number Range', 'Method','Best Method'\n",
    "df_seq_range_count= (df_unique_results [x].groupby(df_unique_results [hue]).value_counts().rename(y).reset_index())\n",
    "df_seq_range_count = df_seq_range_count.rename(columns={\"level_1\":x})\n",
    "num_seq_order = df_seq_range_count.loc[df_seq_range_count['Method']>10][x].sort_values().unique()\n",
    "\n",
    "\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "gc = sns.barplot(x=x, y=y, hue=hue,order = num_seq_order, data=df_seq_range_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "gc.set_yscale('log')\n",
    "ax.set_ylabel('Protein Count', fontsize=14)\n",
    "ax.set_xlabel('Sequence Length', fontsize=14)\n",
    "plt.savefig('BMhist_ncol_range.pdf')\n",
    "\n",
    "plt.show()\n",
    "# bin AUC values for each method\n",
    "plt.figure(figsize=(5.0,5.))\n",
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "gc = sns.barplot(x=x, y=y, hue=hue,order = num_seq_order, data=df_seq_range_count,ax=ax, palette = [\"#2258A5\", \"#FF8B00\",\"#349C55\"])\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "gc.set_yscale('log')\n",
    "ax.set_ylabel('Protein Count', fontsize=20)\n",
    "ax.set_xlabel('Sequence Length', fontsize=20)\n",
    "xlabels = ax.get_xticklabels()\n",
    "ax.set_xticklabels(xlabels, rotation=20, ha='right')\n",
    "plt.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('BMhist_ncol_range_poster.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 proteins from each sequence-count range\n",
    "x,y,hue = 'Effective Sequences (log)', 'Method','Best Method'\n",
    "\n",
    "ex_pdb_ids = []\n",
    "ex_pfam_ids = []\n",
    "ex_aucs = []\n",
    "ex_nseq = []\n",
    "# print(df_unique_results.groupby(df_unique_results[x]).head())\n",
    "print(df_unique_results[x].unique())\n",
    "for eff_seq_range in df_unique_results[x].unique():\n",
    "    top100_range = df_unique_results.loc[df_unique_results[x] == eff_seq_range].sort_values('AUC', ascending=False)[:100]\n",
    "    ex_pdb_ids.extend(top100_range['PDB ID'].values)\n",
    "    ex_pfam_ids.extend(top100_range['Pfam ID'].values)\n",
    "    ex_aucs.extend(top100_range['AUC'].values)\n",
    "    ex_nseq.extend(top100_range['Effective Sequences'].values)\n",
    "\n",
    "print(len(ex_pdb_ids))\n",
    "print(len(ex_pfam_ids))\n",
    "print(len(ex_aucs))\n",
    "print(len(ex_nseq))\n",
    "\n",
    "print(ex_pdb_ids[:10])\n",
    "print(ex_pfam_ids[:10])\n",
    "print(ex_aucs[:10])\n",
    "print(ex_nseq[:10])\n",
    "print(len(np.unique(ex_pfam_ids)))\n",
    "print(len(np.unique(ex_pdb_ids)))\n",
    "\n",
    "# write swarm file for individual plots\n",
    "f = open('single_protein_plots.swarm','w')\n",
    "for i, pdb_id  in enumerate(ex_pdb_ids):\n",
    "    pfam_id = ex_pfam_ids[i]\n",
    "    auc = ex_aucs[i]\n",
    "    nseq = ex_nseq[i]\n",
    "    f.write('source /data/cresswellclayec/conda/etc/profile.d/conda.sh; ')\n",
    "    f.write('conda activate PYDCA; ')\n",
    "    f.write('python single_protein_plot_pydca.py %s %s %s %s $SLURM_CPUS_PER_TASK\\n'%(pdb_id, pfam_id,floor(100*auc),floor(nseq)))\n",
    "    #f.write('module load singularity; ')\n",
    "    #f.write('singularity exec -B /data/cresswellclayec/DCA_ER/biowulf/ /data/cresswellclayec/DCA_ER/dca_er.simg python 1main_ER.py %s\\n'%(pdb))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique_results.loc[df_unique_results['Pfam ID']=='PF00196'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ecdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique_results.loc[df_unique_results['Best Method']=='ER'].sort_values('AUC', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92b236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
